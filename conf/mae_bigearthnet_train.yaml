trainer:
  accelerator: "gpu"
  devices: [0,1]
  precision: 32
  accumulate_grad_batches: 1
  max_epochs: 800
  limit_train_batches: 50
  # log_every_n_steps: 1
  limit_val_batches: 0 #40
  check_val_every_n_epoch: 10
  benchmark: True
  enable_progress_bar: True
  fast_dev_run: False
experiment:
  task: "mae_bigearthnet_train"
  name: "mae_bigearthnet_train"
  run:
    fit: True
    test: False
  module:
    model: "mae"
    encoder_name: "vit"
    sensor: "naip"
    imagenet_pretrained: False
    pretrained: False
    # resume_checkpoint: "epoch=639-step=41600.ckpt"
    image_size: 120
    crop_size: 96
    patch_size: 8
    batch_size: ${experiment.datamodule.batch_size} 
    create_sharded: True
    channel_wise: True
    channel_shuffle: True
    mask_tokens_encoder: False
    mask_tokens_decoder: True
    mask_tokens_reduction_encoder: False
    mask_tokens_reduction_decoder: True
    norm_pix_loss: False
    num_in_channels: 12
    num_out_channels: 12
    num_checkpoints_encoder: 0
    num_checkpoints_decoder: 0
    mlp_ratio: 1.0
    embed_dim: 1024
    depth: 4
    num_heads: 2
    decoder_embed_dim: 1024
    decoder_depth: 2
    decoder_num_heads: 1
    mask_fns:
      - "random_masking_ratio"
      - "random_masking"
    mask_kwargs:
      random_masking_ratio:
        ratio: 0.7
        probability: 1.0
      random_masking:
        num_keep: 225
        probability: 1.0
    lr: 1.5e-4
    lr_min: 0.0
    warmup_lr_init: 1.5e-7
    num_warmup: 2
  datamodule:
    root_dir: "/data/users/mike/data/BigEarthNetStacked"
    #root_dir: "/data/users/mike/data/FFCV"
    bands: "all"
    num_classes: 19
    batch_size: 64
    num_workers: 8
    pin_memory: True
    prefetch_factor: 2
    persistent_workers: True
    load_target: False
    use_ffcv: False
    distributed: False
    batches_ahead: 3
logger:
  name: "wandb"
  offline: False
  project_name: "master-thesis"

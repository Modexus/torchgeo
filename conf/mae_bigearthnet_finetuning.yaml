trainer:
  accelerator: "gpu"
  devices: [0,1,2,3]
  precision: "bf16"
  accumulate_grad_batches: 16 #8 #4
  max_epochs: 50
  limit_train_batches: 1.0
  check_val_every_n_epoch: 5
  benchmark: True
  fast_dev_run: False
experiment:
  task: "mae_bigearthnet_finetuning"
  name: "mae_bigearthnet_finetuning"
  run:
    fit: True
    test: False
  module:
    task_name: "mae"
    model: "mae"
    encoder_name: "vit"
    sensor: "naip"
    bands: "all"
    image_size: 120
    crop_size: 96
    patch_size: 8
    # resume_checkpoint: "last.ckpt"
    batch_size: ${experiment.datamodule.batch_size}
    mean_patches: True
    channel_wise: True
    multi_label: True
    in_channels: 10
    out_channels: 1
    num_classes: 19
    load_checkpoint: "epoch=399-step=38400-v2.ckpt"
    imagenet_pretrained: False
    optimizer: "ADAMW"
    lr: 1e-3
    optimizer_kwargs:
      weight_decay: 0.05
      betas: 
        - 0.9
        - 0.999
    lr_min: 0.0
    warmup_lr_init: 0.0
    num_warmup: 5
    mask_fns:
      - "random_channel_masking"
    mask_kwargs:
      random_channel_masking:
        num_keep: 432 # 200 #112
        probability: 1.0
    # mask_num_channels: 3
  datamodule:
    # root_dir: "/scratch/users/mike/data/BigEarthNet"
    root_dir: "/scratch/users/mike/data/FFCV"
    bands: "all"
    num_classes: 19
    batch_size: 64 #128 #256
    num_workers: 7
    pin_memory: True
    prefetch_factor: 5
    persistent_workers: True
    load_target: True
    use_ffcv: True
    distributed: True
    batches_ahead: 4
logger:
  name: "wandb"
  offline: False
  project_name: "master-thesis"
